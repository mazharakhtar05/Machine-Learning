# Model Evaluation using Metrics and Confusion Matrix
# ---------------------------------------------------
# This program demonstrates how to evaluate ML model predictions
# using Accuracy, Precision, Recall, F1-Score, and Confusion Matrix.

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# -----------------------------
# True labels (actual answers)
# -----------------------------
y_true = [1, 0, 1, 1, 0, 1, 0]

# Model predictions (guessed results)
y_pred = [1, 0, 1, 0, 0, 1, 1]

# -----------------------------
# Evaluation Metrics
# -----------------------------
print("ðŸ“Š Model Evaluation Metrics")
print("Accuracy  :", accuracy_score(y_true, y_pred))
print("Precision :", precision_score(y_true, y_pred))
print("Recall    :", recall_score(y_true, y_pred))
print("F1 Score  :", f1_score(y_true, y_pred))

# -----------------------------
# Confusion Matrix Example
# -----------------------------
y_true_cm = [1, 0, 0, 1, 1, 0, 0, 1]
y_pred_cm = [1, 0, 0, 1, 0, 1, 0, 0]

cm = confusion_matrix(y_true_cm, y_pred_cm)
print("\nðŸ§® Confusion Matrix:\n", cm)
